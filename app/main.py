import os
from fastapi import FastAPI
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
from openai import OpenAI

# FastAPI ì•± ìƒì„±
app = FastAPI()

# ì…ë ¥ ìŠ¤í‚¤ë§ˆ
class CodeRequest(BaseModel):
    code: str

# ëª¨ë¸ ê²½ë¡œë¥¼ Hugging Face ê²½ë¡œë¡œ ì„¤ì •
model_path = "davin0706/codebert-finetuned-v5"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

# âœ… ì ìˆ˜ ë§¤í•‘
score_map = {
    0: {"label": "SQL_Injection", "score": 30, "msg": "âš ï¸ SQL Injection ì·¨ì•½ì  ê°ì§€"},
    1: {"label": "Hardcoded_Password", "score": 30, "msg": "âš ï¸ Hardcoded Password ê°ì§€"},
    2: {"label": "XSS", "score": 30, "msg": "âš ï¸ XSS ì·¨ì•½ì  ê°ì§€"},
    3: {"label": "Safe_Code", "score": 100, "msg": "âœ… ì™„ì „í•œ ë°©ì–´ ì½”ë“œ (Safe Code)"},
    4: {"label": "Other", "score": 70, "msg": "âš ï¸ ì ì¬ì  ì·¨ì•½ì ì´ ìˆëŠ” ì½”ë“œ (Other)"}
}

# âœ… ë³´ì•ˆ ë¶„ì„ í•¨ìˆ˜
def analyze_code(code_snippet: str):
    inputs = tokenizer(code_snippet, return_tensors="pt", truncation=True, padding="max_length", max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = F.softmax(logits, dim=1)[0]
        predicted = torch.argmax(probs).item()

        # Safe vs Other í›„ì²˜ë¦¬
        if predicted == 3:
            if probs[4].item() > 0.3 and probs[3].item() < 0.7:
                predicted = 4

    return {
        "prediction": score_map[predicted]["msg"],
        "label": score_map[predicted]["label"],
        "security_score": score_map[predicted]["score"]
    }

# âœ… GPT ë¦¬í¬íŠ¸ ìƒì„± í•¨ìˆ˜
def generate_report(code_snippet: str, label: str) -> str:
    prompt = f"""
ë‹¹ì‹ ì€ ë³´ì•ˆ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ ì†ŒìŠ¤ ì½”ë“œì—ì„œ ê°ì§€ëœ ì·¨ì•½ì ì€ **'{label}'** ì…ë‹ˆë‹¤.

[ì·¨ì•½ ì½”ë“œ]
{code_snippet.rstrip()}


ğŸ“Œ **1. ì·¨ì•½ì  ì„¤ëª…**
í•´ë‹¹ ì·¨ì•½ì ì´ ë°œìƒí•œ ì´ìœ ì™€ ì½”ë“œ êµ¬ì¡°ìƒ ë¬¸ì œì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ğŸ’£ **2. ê³µê²© ì‹œë‚˜ë¦¬ì˜¤**
ê³µê²©ìê°€ í•´ë‹¹ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì•…ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ğŸ›  **3. ë³´ì™„ ë°©ë²• ë° ê°œì„ ëœ ì½”ë“œ ì˜ˆì‹œ**
ë³´ì™„ ë°©ë²•ì„ ì„¤ëª…í•˜ê³ , ë³´ì™„ëœ ì½”ë“œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

âœ… **4. ìš”ì•½ëœ ë³´ì•ˆ ê¶Œê³ ì‚¬í•­**
ìš”ì•½ëœ ê¶Œê³ ì‚¬í•­ì„ **ë¦¬ìŠ¤íŠ¸ í˜•ì‹**ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

---

ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ğŸ“„ **ë³´ì•ˆ ë¶„ì„ ë¦¬í¬íŠ¸**

---
"""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant for code security analysis."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=1200
    )
    return response.choices[0].message.content

# âœ… API ì—”ë“œí¬ì¸íŠ¸
@app.post("/analyze")
def analyze(request: CodeRequest):
    result = analyze_code(request.code)
    gpt_report = generate_report(request.code, result["label"])
    return {
        "prediction": result["prediction"],
        "label": result["label"],
        "security_score": result["security_score"],
        "report": gpt_report
    }
